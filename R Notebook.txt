a<-11
a
a<-"pune"   # everything which is code in " " is string


num1 <- 14
num1+10

######combine two or more numbers######

num2 <- c(10,11,12)
num3 <- c(110,111,112)
num2+num3

### data frame ###

id <- c(1,2,3,4,5,6,7,8,9,10)
age <- c(30,31,34,40,41,51,21,29,25,26)
salary <-c(100,101,102,103,104,105,1000,1100,500,600)
emply_data <- data.frame(id, age, salary)
emply_data

#### to see the top 6 records ####

head(emply_data)

#### to see the last 6 records ####

tail(emply_data)

#### to see summary (mean,median,min,max) ####

summary(emply_data)

#### select rows and columns that u wants ####

emply_data[c(1:4) , c(1,2,3)]
emply_data[c(1:4) , ]

#### view data ( V Capital) ####

View(emply_data)

View(iris)

dim(iris)  # for knowing no of colms and rows

iris1 <- iris

#### To select first 25, 80 to 100, 110 to 150 ####

iris1[c(1:25 , 80:100 , 110:150) , ]

summary(iris1)

View(mtcars)
summary(mtcars)

colnames(iris1)

nrow(iris1)
ncol(iris1)

#### for selecting particular column ####

emply_data$id
mean(emply_data$id)

#### adding new column as experience using cbind ####

experience <- c(2,3,4,6,2,4,10,11,12,14)
emply_data1 <- cbind(emply_data, experience)
emply_data1

#### adding new row as new emply using rbind ####

new_emply <- c(11 , 34 , 1000 , 14)
emply_data1 <- rbind(emply_data1 , new_emply)
emply_data1

#### delet dataset ####

rm(emply_data1)
emply_data1

#### if we want to delet particular column ####

emply_data[c(1:11) , -4]


gender <- c(1,1,1,1,0,0,0,1,0,1)
emply_data1 <- cbind(emply_data , gender)
emply_data1

summary(emply_data1)

#### for factorise gender only means how many male and female(divide in two categories) ####

emply_data1$gender <- factor(emply_data1$gender)
summary(emply_data1)

#### change column name ####

colnames(emply_data1)[2] <-"new_age"
emply_data1

#### change two colms name ####

colnames(emply_data1)[c(1,3)] <- c("new_id","new_salary")
emply_data1

#### corr matrix ####

cor(mtcars)

#### corr for iris1 data ####

cor(iris1[, c(1:4)])

# or

iris2<- iris1[, c(1:4)]
cor(iris2)

#### reading external data file ####

CR <- read.csv("C:/Users/DC/Desktop/Whatsapp/CreditRisk.csv")
CR
dim (CR)

View (CR)

summary(CR)

#### analysis of applicant income ####

quantile(CR$ApplicantIncome , probs = c(.1, .2, .3, .4, .5, .6, .7, .8, .9, 1))


#### using filter ####

library(dplyr)
filter_exm <- filter(mtcars, disp == 160)
filter_exm

df1 <- filter(CR, (ApplicantIncome > 25000) & (Gender == "Male"))
df1

df2 <- select (CR, ApplicantIncome, Gender, Education)
df2


#### Mutate function ( to add a new column) ####

library (dplyr)
CR <- mutate(CR , Loan_Status1 = ifelse(Loan_Status == "Y",1,0))
CR  

# OR 

library (dplyr)
CR <- mutate(CR , Loan_Status1 = ifelse(Loan_Status == "N",0,1))
CR 

library (dplyr)
CR <- mutate(CR , newarea = ifelse(Property_Area == "Urban",2, ifelse (Property_Area =="Rural",0,1)))
CR

CR$newarea <- factor(CR$newarea)
CR

CR$married <- as.factor(as.numeric(CR$Married))  
CR

summary(CR)

#### use na.omit for omit column or row whoose contain missing values ####

CR <- na.omit(CR)
CR

summary(CR)

#### Sampeling (Train & Test) ####

CR <- read.csv("C:/Users/DC/Desktop/Whatsapp/CreditRisk.csv")
CR

Sample_CR <- sample( 2, nrow(CR) , replace = TRUE , prob = c(.8,.2))
Sample_CR

CR_train <- CR[Sample_CR == 1,]
CR_test <- CR[Sample_CR ==2,]

dim(CR_train)
dim(CR_test)


#### table function (variable info) ####

table(CR$Gender)

table(CR$Loan_Status)

#### Two dimensional or cross tabulation ####

table(CR$Gender, CR$Loan_Status)

table(mtcars$am , mtcars$gear)

#### arrange function ####

# ascending order

library(dplyr)
arrange(mtcars,disp)

# descending order

arrange(mtcars,desc(disp))

#### matrix ####

aa <- c(45,2,3,55,6,99,577,442,234,20)
matt <- matrix(aa, nrow=2, ncol=5)  # by column default
matt

matt1 <- matrix(aa, nrow=2, ncol=5, byrow = TRUE) #by row
matt1

a <- c(7,3,4,5)
b <- c(1,11,13,67)

mat1 <- matrix(a,2,2)


mat2 <- matrix(b,2,2)

det(mat1)
det(mat2)

#### solving eqn ####

#a+2b+2c = 20
#2a+5b+8c = 80
#6a+7b+9c = 190

#Ans


coff <- c(1,2,2,2,5,8,6,7,9)
values <- c(20,80,190)
AA <- matrix(coff, nrow = 3, ncol = 3, byrow = TRUE)
BB <- matrix(values,nrow=3, ncol=1)
abc <- solve(AA,BB)
abc

#### Array (collection of matrix) ####

vector1 <- c(5,9,3)
vector2 <- c(10,11,12,13,14)
result <- array(c(vector1,vector2),dim = c(2,2,2))
result

vector3 <- c(5,9,3,10,11,12,13,14)
result <- array(c(vector3),dim = c(2,2,2))
result

result1 <- array(c(vector3),dim = c(2,2,3))  #dim = c(no of row, no of col, no of matrix)
result1

#### convert dataframe into matrix ####

CR <- read.csv("C:/Users/DC/Desktop/Whatsapp/CreditRisk.csv")
CR
class(CR)
CR <- as.matrix(CR)
CR

#### convert matrix into dataframe ####

class(CR)
CR <- as.data.frame(CR)
CR

#### list (list is collection of any form of data matrix,vector,df,list,...) ####

list1 <- list(CR,mat1,mat2)
list1

############################## Data types are over #################################


#### Charts ####

# 1. Scatter plot (independent values on X axis & dependent values n Y axis)

num1 <- c(1,2,3,4,5,6,7,8)
plot(num1)

# 2.

squr <- c(1,4,9,16,25,36,49,64)
plot(num1,squr)

plot(mtcars$cyl, mtcars$disp, main = 'cyl vs disp', xlab = 'cyl', ylab = 'disp',col = 'red') # (col for colour)

# boxplot

boxplot(mtcars$disp)

CR <- read.csv("C:/Users/DC/Desktop/Whatsapp/CreditRisk.csv")
CR

abc=boxplot(CR$ApplicantIncome)
abc

# Pie chart

a = 10
b = 20
paste(a,b)

GDP <- c(19.4,11.8,4.8,3.4,2.5,2.4)
countries <- c('US','CHINA','JAPAN','GERM','UK','INDIA')
GDP_PIE <- pie(GDP,labels = countries,main = 'gdp distribution')
Total <- sum(GDP)
Total

GDP_Percent <- GDP / Total * 100
GDP_Percent <- round(GDP_Percent,1)
GDP_PIE <- pie(GDP,labels = paste(countries,GDP_Percent),main = 'gdp distribution')



# histogram

hist(mtcars$disp,breaks=20,col='gray')
plot(density(mtcars$disp))  #Kernel density plot

# Bar Plot

abc=table(mtcars$gear)
abc
barplot(abc)

barplot(abc, xlab = "number of gear",ylab = "count of cars by gear", main= "Gear Vs Count" ,col= c("orange","green","red"),legend = rownames(abc))

#### Conditional statements ####

# 1.if else statement

x <- -4

if (x>0)
{
  
  print("Positive Number")
  
}else
{
  print("Negative Number")
}

#2.Nested if else statement

aa <- 9
if(aa < 10)
{
  print ("inside the if part")
  
}else if(aa == 10){
  print("inside the else if")
}else
{
  print("inside the else part")
}

#### Loops ####

#1. while loop

i = 1
while(i < 10){
  print(i)
  i = i+1
}

#2. repeat OR (if break) loop

a = 1
repeat{
  print(a)
  a = a+1
  if (a>4)
    break
}

#3. for loop

cities = c("mumbai","pune","dilhi")
for(i in cities){
  print(i)
  
}

######################## Que. print 1 to 50 numbers and filter it odd and even #############################

i = 1
while(i < 51){
  print(i)
  i = i+1
}

num <- (1:50)
num

#### user defined functions ####

Add_Num <-function(a1,a2){
  a3 <<- a1 + a2  # Global variable (it will print outside the function)
  ab = a1 - a2
  print (ab)
  print (a3)
}
Add_Num(20 ,30)


# QUE.

fact <- function(a){
  a > 0
  fact(a)
  print (a)
  
}
fact(5)

#### apply function (data name, row =1, col =2, function name) #### 

iris1
iris2 = iris1[c(1:150) , -5]
apply(iris2,2,mean)

# Or 

apply(iris1[,c(1:4)],2,mean)

#### tapply (tiered) ####

age = c(30,31,32,33,34,35)
gender =  c('m','f','m','m','f','f')
tapply(age,gender,mean)

#### aggregate function ####

aggregate(iris1[,c(1:4)],list(iris1[,5]),mean)

#### working directory ####

getwd()


#### Build model ####

mtcars
model1 <- lm(mpg~.,data=mtcars)  # mpg is target variable, . means it takes all remain variable as independent variables
summary(model1)

model2 <- lm(mpg~+wt+am+qsec+hp+disp, data=mtcars)
summary(model2)

predicted <- predict(model2,mtcars)

predicted_actual <- data.frame(predicted, data=mtcars$mpg)
predicted_actual
colnames(predicted_actual) <- c("predicted","actual")
predicted_actual

MTCARS1 <- read.csv("C:/Users/DC/Desktop/Whatsapp/MTCARS1.csv")
MTCARS1
model1 <- lm(mpg~.,data=MTCARS1)  # mpg is target variable, . means it takes all remain variable as independent variables
summary(model1)

View (MTCARS1)

dim (MTCARS1)

MTCARS2 <- MTCARS1[c(1:32) , -13]
MTCARS2

summary (MTCARS2)
model1 <- lm(mpg~.,data=MTCARS2)
summary (model1)

LungCapData <- read.csv("C:/Users/DC/Desktop/Whatsapp/LungCapData.csv")
LungCapData

head(LungCapData)
summary(LungCapData)

model1 <- lm(LungCap~.,data=LungCapData)  # lung capacity is target variable
summary (model1)

LungCapData <- read.csv("C:/Users/DC/Desktop/Whatsapp/LungCapData.csv")
LungCapData
library(dplyr)

LungCapData <- mutate(LungCapData , Smoke1 = ifelse(Smoke == "no",0,1))
LungCapData
LungCapData <- mutate(LungCapData , Gender1 = ifelse(Gender == "male",1,0))
LungCapData
LungCapData <- mutate(LungCapData , Caesarean1 = ifelse(Smoke == "no",0,1))
LungCapData
head (LungCapData)
LCN <- LungCapData[, c(1,2,3,7,8,9)]
LCN

summary(LCN)

LCN$Smoke1 <- factor(LCN$Smoke1)
LCN$Gender1 <- factor(LCN$Gender1)
LCN$Caesarean1 <- factor(LCN$Caesarean1)
summary(LCN)

LCN_Sam = sample(2,nrow(LCN),replace = TRUE,prob = c(.8,.2))
LCN_Train = LCN[LCN_Sam==1,]
LCN_Test = LCN[LCN_Sam==2,]
lcn_model=lm(LungCap~.,data=LCN_Train)
summary(lcn_model)

pred <- predict(lcn_model , LCN_Test$LungCapData)
pred
pred_actual <- data.frame(pred , LCN_Train) 
pred_actual
colnames(pred_actual) <- c("predicted" , "actual")



diff <- pred_actual$predicted - pred_actual$actual
diff
pred_actual_diff <- data.frame(pred_actual , diff)
pred_actual_diff


summary((pred_actual_diff))





##### boxplot ######

abc <- boxplot(LungCap~Age,data=LCN)
abc

LCN <-  filter(LCN, LungCap != c(1.850,1.625,10.200,3.650,11.950,3.450,5.675,6.150,14.375,14.550))
LCN

model1 <- lm(LungCap~.,data=LungCapData)  # lung capacity is target variable
summary (model1)


summary (LCN)







#######Logistic Regression#########

Credit_Risk=read.csv("C:/Users/DC/Desktop/Whatsapp/CreditRisk.csv", na.strings = "")

##1.problem statement
##2. data cleaning
##3. sampling
##4. build the model
##5. test

View(Credit_Risk)
head(Credit_Risk)
dim(Credit_Risk)
summary(Credit_Risk)


## how to deal with NULLs

##finding mean after removeing NA

var1<- mean(Credit_Risk$LoanAmount,na.rm = TRUE)
##with is.na(dataframe$column) to check if there are any null values or na
##Replacing na with var1
Credit_Risk$LoanAmount[is.na(Credit_Risk$LoanAmount)]<-var1

var3<- mean(Credit_Risk$ApplicantIncome,na.rm = TRUE)
Credit_Risk$ApplicantIncome[is.na(Credit_Risk$ApplicantIncome)]<-var3

var4<- mean(Credit_Risk$CoapplicantIncome,na.rm = TRUE)
Credit_Risk$CoapplicantIncome[is.na(Credit_Risk$CoapplicantIncome)]<-var4

Credit_Risk$Credit_History[is.na(Credit_Risk$Credit_History)]<-0

##
Credit_Risk <- na.omit(Credit_Risk)

##Credit_Risk$Self_Employed[is.na(Credit_Risk$Self_Employed)]<-0

summary(Credit_Risk)
dim(Credit_Risk)

##factoring main dataframe
Credit_Risk<- mutate(Credit_Risk, Loan_Status1 = ifelse(Loan_Status == "Y",1,0))
Credit_Risk<- mutate(Credit_Risk, Education1 = ifelse(Education == "Graduate",1,0))
Credit_Risk<- mutate(Credit_Risk, Self_Employed1 = ifelse(Self_Employed == "Yes",1,0))
Credit_Risk<- mutate(Credit_Risk, newarea1 = ifelse(Property_Area == "Urban",2,ifelse(Property_Area=="Rural",0,1)))
Credit_Risk<- mutate(Credit_Risk, Gender1 = ifelse(Gender == "Female",1,0))
Credit_Risk<- mutate(Credit_Risk, Married1 = ifelse(Married == "No",1,0))

dim(Credit_Risk)
summary(Credit_Risk)
head(Credit_Risk)

##keeping required columns from main dataframe
Credit_Risk <- select(Credit_Risk,4,7,8,9,10,11,14,15,16,17,18,19)
dim(Credit_Risk)
head(Credit_Risk)

##Factor the variables

Credit_Risk$Loan_Status1 = factor(Credit_Risk$Loan_Status1)
Credit_Risk$Education1 = factor(Credit_Risk$Education1)
Credit_Risk$Self_Employed1 = factor(Credit_Risk$Self_Employed1)
Credit_Risk$newarea1 = factor(Credit_Risk$newarea1)
Credit_Risk$Gender1 = factor(Credit_Risk$Gender1)
Credit_Risk$Married1 = factor(Credit_Risk$Married1)

summary(Credit_Risk)

##Sampling
crs<- sample(2,nrow(Credit_Risk),replace = TRUE,prob = c(.8,.2))
train_crs <- Credit_Risk[crs == 1,]
test_crs <- Credit_Risk[crs == 2,]
dim(train_crs)
dim(test_crs)
summary(train_crs)
##Building Model
##in logistic regression we have to always specify family=binomial ie having only two values
##Loan_Status1 is target veriable
model_cr1 <- glm(Loan_Status1 ~.,family = binomial,data=train_crs)


##Prediction
pred_val  <- predict(model_cr1, test_crs, type="response")
pred_val

##Keeping predicted values in dataframe
pred_actual_df<-data.frame(pred_val,test_crs$Loan_Status1)

##converting it in 0 and 1
pred_actual_df<-mutate(pred_actual_df,pred_val=ifelse(pred_val>0.5,1,0))

##rename the columns
colnames(pred_actual_df) <- c('Predicted','Actual')


#confusion matrix 
tab = table(pred_actual_df$Predicted,pred_actual_df$Actual)
tab

##matrix
#   0   1
#0  24  6
#1  26  133

Acc = (24+133)/(24+133+26+6) ##manual calculation of accuracy


sum(diag(tab))/sum(tab) ## calculation with formula


summary(pred_actual_df)

##################################################################################################################
library(dplyr)

## do prediction on entire data
## df(pred,actual,prob,loanid)
##df(sort, desc(prob))

Credit_Risk=read.csv("C:/Users/DC/Desktop/Whatsapp/CreditRisk.csv", na.strings = "")

##1.problem statement
##2. data cleaning
##3. sampling
##4. build the model
##5. test

View(Credit_Risk)
head(Credit_Risk)
dim(Credit_Risk)
summary(Credit_Risk)


## how to deal with NULLs

##finding mean after removeing NA

var1<- mean(Credit_Risk$LoanAmount,na.rm = TRUE)
##with is.na(dataframe$column) to check if there are any null values or na
##Replacing na with var1
Credit_Risk$LoanAmount[is.na(Credit_Risk$LoanAmount)]<-var1

var3<- mean(Credit_Risk$ApplicantIncome,na.rm = TRUE)
Credit_Risk$ApplicantIncome[is.na(Credit_Risk$ApplicantIncome)]<-var3

var4<- mean(Credit_Risk$CoapplicantIncome,na.rm = TRUE)
Credit_Risk$CoapplicantIncome[is.na(Credit_Risk$CoapplicantIncome)]<-var4

Credit_Risk$Credit_History[is.na(Credit_Risk$Credit_History)]<-0

##
Credit_Risk <- na.omit(Credit_Risk)

##Credit_Risk$Self_Employed[is.na(Credit_Risk$Self_Employed)]<-0

summary(Credit_Risk)
dim(Credit_Risk)

##factoring main dataframe
Credit_Risk<- mutate(Credit_Risk, Loan_Status1 = ifelse(Loan_Status == "Y",1,0))
Credit_Risk<- mutate(Credit_Risk, Education1 = ifelse(Education == "Graduate",1,0))
Credit_Risk<- mutate(Credit_Risk, Self_Employed1 = ifelse(Self_Employed == "Yes",1,0))
Credit_Risk<- mutate(Credit_Risk, newarea1 = ifelse(Property_Area == "Urban",2,ifelse(Property_Area=="Rural",0,1)))
Credit_Risk<- mutate(Credit_Risk, Gender1 = ifelse(Gender == "Female",1,0))
Credit_Risk<- mutate(Credit_Risk, Married1 = ifelse(Married == "No",1,0))

dim(Credit_Risk)
summary(Credit_Risk)
head(Credit_Risk)

##keeping required columns from main dataframe
Credit_Risk <- select(Credit_Risk,1,4,7,8,9,10,11,14,15,16,17,18,19)
dim(Credit_Risk)
head(Credit_Risk)

##Factor the variables

Credit_Risk$Loan_Status1 = factor(Credit_Risk$Loan_Status1)
Credit_Risk$Education1 = factor(Credit_Risk$Education1)
Credit_Risk$Self_Employed1 = factor(Credit_Risk$Self_Employed1)
Credit_Risk$newarea1 = factor(Credit_Risk$newarea1)
Credit_Risk$Gender1 = factor(Credit_Risk$Gender1)
Credit_Risk$Married1 = factor(Credit_Risk$Married1)

summary(Credit_Risk)

##Sampling
crs<- sample(2,nrow(Credit_Risk),replace = TRUE,prob = c(.8,.2))
train_crs <- Credit_Risk[crs == 1,]
test_crs <- Credit_Risk[crs == 2,]
dim(train_crs)
dim(test_crs)
summary(train_crs)
##Building Model
##in logistic regression we have to always specify family=binomial ie having only two values
##Loan_Status1 is target veriable
model_cr1 <- glm(Loan_Status1 ~.,family = binomial,data=train_crs[,-1])
summary(model_cr1)
##Prediction
pred_val_prob  <- predict(model_cr1, Credit_Risk, type="response")
pred_val_prob

##Keeping predicted values in dataframe
pred_actual_df<-data.frame(Credit_Risk$Loan_ID, pred_val_prob,Credit_Risk$Loan_Status1)

##converting it in 0 and 1
pred_actual_df<-mutate(pred_actual_df,pred_val=ifelse(pred_val_prob>0.5,1,0))

##sorting dataframe
pred_actual_dfSorted<-arrange(pred_actual_df,desc(pred_val_prob))

View(pred_actual_dfSorted)
##rename the columns
colnames(pred_actual_dfSorted) <- c('LoanID','Probability','Actual','Predicted')


#confusion matrix 
tab = table(pred_actual_dfSorted$Predicted,pred_actual_dfSorted$Actual)
tab

##matrix
#   0   1
#0  24  6
#1  26  133

Acc = (24+133)/(24+133+26+6) ##manual calculation of accuracy


sum(diag(tab))/sum(tab) ## calculation with formula


summary(pred_actual_df)

##glm(y ~ . , data =     df[ , c(1:10 , 15 :20)])

#-----------------------------------------------------------------------------------------------------

#Not Succesful for Party hence installed 'rpart' instead.

library(rpart)

#LB : beats per seconds
#AC : accelerationper second
#FM : fetal movement per second

#NSP : (Normal, Suspect, pathalogical)

ctg =read.csv("C:/Users/DC/Desktop/Whatsapp/CTG.csv")
ctg

ctg$NSP = factor(ctg$NSP)


ctg_sample = sample(2,nrow(ctg),replace = TRUE,prob=c(.8,.2))
ctg_train=ctg[CRS==1,]
ctg_test=ctg[CRS==2,]

ctg_dt=rpart(NSP~LB+AC+FM,data=ctg_train)
ctg_dt

pred=predict(ctg_dt,ctg_test,type='class')
pred

#prediction on full data

pred1=predict(ctg_dt,ctg,type='class')
pred1

ctg_pred_act_diff = data.frame(pred1, data=ctg$NSP)

colnames(ctg_pred_act_diff)= c('predicted','actual')
ctg_pred_act_diff



ctg_tab = table(ctg_pred_act_diff$predicted,ctg_pred_act_diff$actual)
ctg_tab

table(ctg$NSP) #to check class imbalances - always check on target variable

########## Decision Tree #########

library(rpart.plot)
library(dplyr)
model1 <- rpart(NSP~LB+AC+FM, data=ctg_train)
model2 <- rpart(NSP~LB+AC+FM,data=ctg_train , minsplit = 500, maxdepth=10)
rpart.plot(model1)
rpart.plot(model2)

# Random Forest

library(randomForest)
modelrf <- randomForest(NSP~.,data=ctg_train, ntree=1000)
modelrf
plot(modelrf)

modelrf_sample = sample(2,nrow(ctg_train),replace = TRUE,prob=c(.8,.2))
modelrf_train=modelrf[modelrfs==1,]
modelrf_test=modelrfs[modelrf==2,]

model_dt=randomForest(NSP~LB+AC+FM,data=modelrf_train)
modelrf_dt

pred=predict(modelrf_dt,modelrf_test,type='class')
pred   ############ incomplete

######## Support Vector Machine (SVM)

library(e1071)
SVM <- svm(NSP~LB+AC+FM, data=ctg_train)
SVM

######### KNN
library(class)
knn_ctg <- knn(train=ctg_train , test = ctg_test ,cl = ctg_train$NSP , k = 5)
knn_ctg


# Naive Bayes

library(e1071)
Naivebays <- naiveBayes(NSP~LB+AC+FM, data=ctg_train)
Naivebays

######################################################################

ctg <- read.csv("C:/Users/DC/Desktop/Whatsapp/CTG.csv")
ctg
library(dplyr)
ctg1 <- filter( ctg ,  ctg$NSP==1)
ctg2 <- unique(ctg1)
ctg3 <- filter( ctg ,ctg$NSP==2 | ctg$NSP==3)
ctg4 <- rbind(ctg2,ctg3)
table(ctg4$NSP)
ctg4$NSP <- factor(ctg4$NSP)
library(randomForest)
modelrf_sample = sample(2,nrow(ctg4),replace = TRUE,prob=c(.8,.2))
modelrf_train=ctg4[modelrf_sample==1,]
modelrf_test=ctg4[modelrf_sample==2,]
modelrf <- randomForest(NSP~.,data=modelrf_train, ntree=250)
modelrf
pred <- predict(modelrf,modelrf_test,type='class')
pred  
ctg_pred_act_diff = data.frame(pred, data=modelrf_test$NSP)
ctg_pred_act_diff
colnames(ctg_pred_act_diff)= c('predicted','actual')
ctg_pred_act_diff

##### confusion matrix

ctg_tab = table(ctg_pred_act_diff$predicted,ctg_pred_act_diff$actual)
ctg_tab
accuracy <- sum(diag(ctg_tab))/sum(ctg_tab)
accuracy

# full data predict

pred_full <- predict(modelrf,ctg4,type='class')
pred_full  
ctg_pred_act_diff = data.frame(pred_full, data=ctg4$NSP)
ctg_pred_act_diff
colnames(ctg_pred_act_diff)= c('predicted','actual')
ctg_pred_act_diff
ctg_tab <- table(ctg_pred_act_diff$predicted,ctg_pred_act_diff$actual)
ctg_tab
accuracy <- sum(diag(ctg_tab))/sum(ctg_tab)
accuracy

######## Boruta & mlbench

library(Boruta)
library(mlbench)
data(Sonar)
View(Sonar)
model_bor <- Boruta(Class~., data = Sonar)
plot <- plot(model_bor)
plot
model_bor$finalDecision

###########

Credit_Risk <- read.csv("C:/Users/DC/Desktop/Whatsapp/CreditRisk.csv", na.strings = "")
Credit_Risk <- na.omit(Credit_Risk)
Credit_Risk
Credit_Risk1 <- Credit_Risk[, -1]
CR_Bor <- Boruta(Loan_Status~., data = Credit_Risk1)
plot <- plot(CR_Bor)
plot
CR_Bor$finalDecision


######### Unsupervised learning OR Clustering  (K-MEANS & DBSCAN ) #######

####### K-Means ############

ctg <- read.csv("C:/Users/DC/Desktop/Whatsapp/CTG.csv")
ctg
ctg_data <- ctg[,-4]
model_ctg_kmeans <- kmeans(ctg_data,3,nstart=1,iter.max = 10)
model_ctg_kmeans
model_ctg_kmeans$centers
model_ctg_kmeans$tot.withinss

######### plot

i <- seq(1:6)
distance = list()

for(j in i){
  model_ctg_kmeans <- kmeans(ctg_data,j,nstart = 1, iter.max = 10)
  distance = append(distance, model_ctg_kmeans$tot.withinss)
  }
plot(i,distance,type='b',xlab = "Num of cluster",ylab= "Total within square distance",col="red")

##### Decision : Optimum number of cluster is 3

plot(ctg_data$LB,ctg_data$FM,col= model_ctg_kmeans$cluster)

#########

model_mtcars_kmeans <- kmeans(mtcars,3,nstart=1,iter.max = 10)
model_mtcars_kmeans

i <- seq(1:6)
distance = list()

for(j in i){
  model_mtcars_kmeans <- kmeans(mtcars,j,nstart = 1, iter.max = 10)
  distance = append(distance, model_mtcars_kmeans$tot.withinss)
}
plot(i,distance,type='b',xlab = "Num of cluster",ylab= "Total within square distance",col="red")

######## DBSCAN (density base clustering) ########

library(dbscan)

ctg_data <- ctg[,-4]
ctg_data_scale <- scale(ctg_data)
ctg_dbscan <- dbscan(ctg_data_scale,eps=.3,minPts=3)
ctg_dbscan

######### AUROC Curve ########

library(ROSE)
Credit_Risk <- read.csv("C:/Users/DC/Desktop/Whatsapp/CreditRisk.csv")

Model_CR1 = glm(Loan_Status1~.,family = binomial ,data = Credit_Risk)
pred_glm <- predict(Model_CR1,Credit_Risk,type = "response")
pred_actual <- data.frame(pred_glm, Credit_Risk$Loan_Status1)
pred_actual <- mutate(pred_actual, prd_val = ifelse(pred_glm > .5,1,0))
colnames(pred_actual)[2] <- "actual"
roc.curve(pred_actual$actual,pred_glm)

########### Hypothesis Testing ###########

## IF p VALUE IS LESS THAN .05 THEN REJECT YOUR NULL HYPO.

CreditRisk=read.csv("C:/Users/DC/Desktop/Whatsapp/CreditRisk.csv", na.strings = "")
CreditRisk

## one sided

t.test(Credit_Risk$ApplicantIncome, mu=3500, conf.level = .95,alternative = "less")  # accept

t.test(Credit_Risk$ApplicantIncome, mu=3500, conf.level = .95,alternative = "greater") # reject

# Two sided

t.test(Credit_Risk$ApplicantIncome, mu=3500, conf.level = .95,alternative = "two.sided") # reject

### Chi-Square Test (Test of independency )

tab1 <- table(CreditRisk$LoanStatus, CreditRisk$Credit_History)
tab1
chisq.test(tab1)

################

library(MASS)
data(survey)
survey
tab2 <- table(survey$Exer, survey$Smoke)
tab2
chisq.test(tab2)

#############

salary_satisfaction=read.csv("C:/Users/DC/Desktop/Whatsapp/salary_satisfaction.csv", na.strings = "")
salary_satisfaction

# H0 : No relation between salary and satisfaction

tab3 <- table(salary_satisfaction$Service, salary_satisfaction$Salary)
tab3
chisq.test(tab3)

# Result: reject null hypo 

######### ANOVA

restuarnt <- read.csv("C:/Users/DC/Desktop/Whatsapp/restuarnt.csv", na.strings="")
restuarnt

# Que. A food chain is testing and want to introduce 3 new items (samosa,vada,paubhaji). They want to know if all
# the 3 items are equally popular for this they pickup 7 different resturant. 
# Each item is made avalable at 7 different restaurant. 

restuarnt_stack <- stack(restuarnt)
restuarnt_stack

colnames(restuarnt_stack) <- c("salescount","dishname")
restuarnt_Anova <- aov(salescount~dishname, data = restuarnt_stack)

summary(restuarnt_Anova)

# Result : Accept the null hypo 

############## hypothesis is over ######################################33

#### PCA #######

mtcars

mydata <- mtcars[,-1]

mydata1 <- scale(mydata, scale=T) # scale is normalise the data
mydata1

mypca <- prcomp(mydata1)
mypca

summary(mypca) 
dim(mypca$x)
cor(mypca$x)
str(mypca)

df1 <- mypca$x
df2 <- cbind(mypca$x,mtcars$mpg)

df3 <- as.data.frame(df2)
df3

model <- lm(V11~PC1+PC2+PC3+PC4+PC5,data=df3) 

summary(model)
colnames(df3)[11] <- "PC11"
df3

########## Ridge regresssion #####

bodyfat <- read.csv("C:/Users/DC/Desktop/Whatsapp/bodyfat.csv", na.strings="")
bodyfat

cor(bodyfat)

library(ridge)

model1 <-lm(Bodyfat~.,data=bodyfat)
model1
summary(model1)

ridgemodel <- linearRidge(Bodyfat~., data=bodyfat)
ridgemodel
summary(ridgemodel)

pred <- predict(ridgemodel,bodyfat)
pred

######### NLP ( Natural Language Processing) ##########


library(tm)

textfile <- readLines("C:/Users/DC/Desktop/Whatsapp/MrModi_Speech_IndependenceDay_20171.txt") ### here lots of spaces are there


textfile <- paste(readLines("C:/Users/DC/Desktop/Whatsapp/MrModi_Speech_IndependenceDay_20171.txt"),collapse=" ")
textfile

textfile <- tolower(textfile) # lower case
textfile

print(stopwords())

textfile2 <- removeWords(textfile,stopwords())
textfile2

bag_of_word1 <- str_split(textfile2," ")
bag_of_word1

str(bag_of_word1)  # check the type of variable

bag_of_word1 <- unlist(bag_of_word1)
str(bag_of_word1)

library(wordcloud)
wordcloud(bag_of_word1, min.freq=2)  # most frequent words

##################################################################################

sms_spam_df <- read.csv("C:/Users/DC/Desktop/Whatsapp/spam1.csv", na.strings="")
sms_spam_df
dim(sms_spam_df)

sms_spam_df <- sms_spam_df[,c(1,2)]
dim(sms_spam_df)

colnames(sms_spam_df) <- c("type", "text")  #rename the columns
head(sms_spam_df)

sms_corpus <- Corpus(VectorSource(sms_spam_df$text)) ## converting into corpus of documents
sms_corpus

inspect(sms_corpus[1:3])  ## just to see the doc

clean_corpus <- tm_map(sms_corpus,tolower)
clean_corpus <- tm_map(clean_corpus,removeNumbers) ## remove the numbers
clean_corpus <- tm_map(clean_corpus, removePunctuation)
clean_corpus <- tm_map(clean_corpus, removeWords, stopwords())
clean_corpus <- tm_map(clean_corpus, stripWhitespace)

################ above 5 lines data is getting clined

inspect(clean_corpus[5:10]) # just check the data 

sms_dtm <- DocumentTermMatrix(clean_corpus) ## term document matrix
dim(sms_dtm)

sms_dtm

#-------------------------- TRAIN and TEST ------------------------------------------

sms_raw_train <- sms_spam_df[1:4169,]                # Initial file
sms_raw_test <- sms_spam_df[4170:6776,]

#---------------------------------------------------------------------------------

sms_dtm_train <- sms_dtm[1:4169,]               # document term matrix
sms_dtm_test <- sms_dtm[4170:6776,]

#---------------------------------------------------------------------------------


sms_corpus_train <- clean_corpus[1:4169]          # corpus
sms_corpus_test <- clean_corpus[4170:6776]

#-------------------------------------------------------------------------------------

five_times_words <- findFreqTerms(sms_dtm_train, 5)
five_times_words

sms_train <- DocumentTermMatrix(sms_corpus_train, control=list(dictionary=five_times_words)) 
dim(five_times_words)
length(five_times_words)

sms_test <- DocumentTermMatrix(sms_corpus_test, control=list(dictionary=five_times_words)) 
dim(five_times_words)
length(five_times_words)

#################################################

convert_count <- function(x){
  y <- ifelse(x>0,1,0)
  y <- factor(y,levels=c(0,1),labels=c("No","Yes"))
  y
}

sms_train <- apply(sms_train,2, convert_count)
sms_test <- apply(sms_test,2, convert_count)

View(sms_train)

#---------------------------------- Model Bulding -----------------------------------------------------

library(e1071)

sms_classifier <- naiveBayes(sms_train, factor(sms_raw_train$type))
sms_classifier

sms_pred <- predict(sms_classifier, sms_test)
sms_pred

sms_pred_diff <- data.frame(sms_pred,sms_raw_test$type)

colnames(sms_pred_diff) <- c("predicted" , "actual")

tab <- table(sms_pred_diff$predicted,sms_pred_diff$actual) 
tab

acc <- sum(diag(tab))/ sum(tab)*100   ### confusion matrix
acc

#---------------------------------- NLP is over -------------------------------

############ Regression plots ( 4 plots) ###################

LungCapData <- read.csv("C:/Users/DC/Desktop/Whatsapp/LungCapData.csv")
LungCapData

head(LungCapData)
summary(LungCapData)

model1 <- lm(LungCap~.,data=LungCapData)  # lung capacity is target variable

plot <- plot(model1)
plot

################################## Box Cox transformation ###############################
# it is used during linear regression when data is not normally distributed or when after building the linear model
# when we do the plot error terms are not normally distributed or even when the assumption of homoscedasticity is fail

library(MASS)

View(cars)

cars_model <- lm(cars$dist ~ cars$speed)
plot(cars_model)

ab <- boxcox(cars$dist ~ cars$speed)
ab

lambda <- ab$x[which(ab$y==max(ab$y))]
lambda

### model

lm_new <- lm(dist^.424~speed,data=cars)
pred <- predict(lm_new,cars)
pred^2.35


View(trees)
library(MASS)
trees_model <- lm(Volume ~ trees+Girth,data=trees)
plot(trees_model)

trees_boxcox <- boxcox(Volume~.,data=trees)

lambda_trees <- tree_boxcox$x[which(tree(tree_boxcox$y == max(tree_boxcox$y)))]
lambda_trees

ab <- boxcox(trees$Volume ~., data= trees)
ab  ########### incomplete

############### Market Basket ###########

library(arules)

data("Groceries")
summary(Groceries)
rules <- apriori(Groceries,parameter=list(supp=.001,conf=.8))
summary(rules)

rules <- apriori(Groceries,parameter=list(supp=.2,conf=.8))
summary(rules)

rules <- apriori(Groceries,parameter=list(supp=.001,conf=1))
summary(rules)

inspect(rules[1:15])

rules2 <- apriori(Groceries,parameter=list(minlen=5,supp=.001,conf=.8))
rules2

rules1 <- apriori(Groceries,parameter=list(minlen=5,supp=.001,conf=.8),appearance=list(rhs="other vegetables",default="lhs"))
summary(rules1)

#--------------------------------------------------------------------------------------------------------------------

Cosmetics <- read.csv("C:/Users/DC/Desktop/Whatsapp/Cosmetics.csv",colClasses="factor")
Cosmetics

rules_Cosmetics <- apriori(Cosmetics,parameter=list(supp=.7,conf=.8))
summary(rules_Cosmetics)

inspect(rules_Cosmetics[1:10])
                                                  
rules_Cosmetics1 <- apriori(Cosmetics,parameter=list(conf=.6,minlen=2,maxlen=3),appearance=list(rhs=c("Concealer=Yes"),default="lhs"))
rules_Cosmetics1
summary(rules_Cosmetics1)

inspect(rules_Cosmetics1)

#-------------------------------------------------------------------------------

iris$Species <- factor(iris$Species)
iris_sample <- sample(2,nrow(iris),replace=TRUE,prob=c(.8,.2))
iris_train <- iris[iris_sample == 1,]
iris_test <- iris[iris_sample == 2,]

install.packages("caret")
library(caret)
install.packages("recipes")

library(caret)
library(recipes)

ControlParameters1 <- trainControl(method="cv",number=5)

library(e1071)

modelrandom_iris_NB <- train(Species~.,data=iris_train,method="naive_bayes",trControl=ControlParameters1,tuneGrid=NULL)

pred <-predict(modelrandom_iris_NB,iris_test)


############################################### Time Series #################################


View(AirPassengers)
AirPassengers

plot(AirPassengers)

ts(AirPassengers)

decompose(AirPassengers)


# make stationary

d1 <- diff(AirPassengers)
plot(d1)


d2 <- diff(d1)
plot(d2)
 
#--------------------

log_ap <- log(AirPassengers)
plot(log_ap)

d1 <- plot(diff(log_ap))  # stationary is achived (lag is 1)

acf(log_ap)
pacf(log_ap)

model <- arima(log_ap,order=c(1,1,0))

prediction <- predict(model,n.ahead = 5)
prediction

prediction$pred <- 2.714^(prediction$pred)
prediction$pred

# cross entropy


























